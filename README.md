# Asymmetrical VR Game AI
This repository contains all of the AI code from my asymmetrical virtual reality (VR) game project I worked on at Virginia Tech, named *Eidolon*. This is a slice of the much larger code base that I wanted to show off on my portfolio. I worked on this project advised by a professor and had some help from other students however I was the sole programmer.

***[Check out this video I posted to my portfolio YouTube channel for a basic gameplay and tech showcase.](https://youtu.be/dneIoCeTpTM?si=D8zD-7PSECiqe2Cy)***

For code samples, I recommend checking out [AvoidCloseVRAction.cs](/Src/Behaviors/Find%20Target%20Actions/AvoidCloseVRAction.cs), as it is one of the more complex scripts that I am proud of, but also feel free to check out the rest of the repository.

## Gameplay Overview
***Eidolon*** is an asymmetrical multiplayer game where one player in VR faces against four players on mouse and keyboard (PC). Gameplay is asymmetrical not only because some players interface with VR devices and others with mouse and keyboard, but also players using different devices have opposing goals.

The ***Eidolon*** arena contains a **relic**, several **consoles**, and two **jails** scattered around. The **relic** is a red cube surrounded by red, green, and blue force field **barriers**. The **consoles** are small colored stands with buttons on top. **Consoles** can be colored red, green, blue, and white and will swap colors every 60 seconds.

At the beginning of the match, three of the PC players are assigned the colors red, green and blue, with the fourth being the **special** player. The PC players with assigned colors may only press buttons on **consoles** with their same color, but they are unable to see the colors of **consoles**. Only the **special** PC player can see the **console** colors, and it is their job to point out the **console** colors to the other PC players. 

The goal of the PC players is to win the game by getting to the **relic**. In order to get to the **relic**, they must first deactivate the colored **barriers** surrounding it. They can do this by pressing the buttons on the **consoles** with colors corresponding to a **barrier** color. An example gameplay sequence might go like this:

- The **special** PC player points out one of the red **consoles** to the red PC player. The red PC player presses the button on the red **console** and the red **barrier** around the **relic** is deactivated. 
- The **special** PC player points out green and blue **consoles** to the green and blue players, and those respective buttons are pressed, causing the green and blue **barriers** to also be deactivated.
- The PC players are able to reach the **relic** because all colored barriers were deactivated and the PC players win the game.

The goal of the VR player is to prevent the PC players from reaching the **relic**. They can do this by either sending all of the PC players to **jail** or by keeping them from reaching the **relic** until the game timer runs out. The VR player has several abilities at their disposal to assist their goal. They can tag PC players to send them to jail and they can shoot a laser beam that will send PC players to jail when they run into it. After being shot, the laser beam remains in place for 10 seconds so it can also be used as a temporary obstacle to make it easier to tag PC players. The VR player can also throw a frisbee that they can teleport to.

## AI Implementation
After difficulty finding enough users to playtest each time we planned, we decided to address this problem by developing AI capable of replacing PC players, thinking that future players would run into similar challenges without a large playerbase.

At their current level, the AI players are capable of fully replacing PC players and winning the game against the VR player. One caveat is that when there is only one AI PC player and three human PC players, the AI will always be the **special** player. We decided that with time constraints it would be too challenging to make AI that understands humans trying to communicate the colored **console** location to it rather than the other way around. The **special** player is normally assigned to a random PC player. 

### Pathfinding
To navigate around the arena, the AI uses a navigation mesh (NavMesh) and the [A* search algorithm](https://en.wikipedia.org/wiki/A*_search_algorithm) for pathfinding. Specifically, it uses the [A* Pathfinding Project](https://arongranberg.com/) by Aron Granberg for its implementation of NavMeshes and A*. We first attempted to use Unity's built-in pathfinding implementation, but quickly found limitations when we needed to edit navigation mesh node traversal costs. 

While pathfinding AI players have several methods of avoiding being sent to jail by the VR player: 

- We modify the cost of traversing the NavMesh nodes nearest to the VR player. Normally A* chooses the path with the smallest distance cost to the target position, but by artificially increasing the distance cost of nodes nearest to the VR player, the AI will choose paths of traversal that automatically avoid getting close to the VR player. Unfortunately, this cannot always prevent the AI from getting close to the VR player. If there is only one possible path to the target or if the VR player is already in the same NavMesh node as the AI then this method will not be able to prevent the VR player from getting too close. This code can be found in [AvoidVROnPathAction.cs](/Src/Behaviors/AvoidVROnPathAction.cs).

- The AI avoids the VR player within a small elliptical radius. The AI considers three ellipses around the VR player, which we call the inner, middle, and outer ellipses. The inner ellipse is what the AI ultimately wants to avoid traversing. If the AI is inside of the inner ellipse or if the current path of the AI intersects with the inner ellipse, then the AI will move along the middle ellipse around the VR player. These calculations are only performed once the AI is inside of the outer ellipse to save performance. If the AI is unable to travel along one side of the middle ellipse then it will change direction. If it is still unable to escape the VR player after a period of time has passed, then the size of the three ellipses will shrink, hopefully allowing the AI to escape at the cost of increased risk of being sent to jail. The foci of the ellipses are aligned with the direction the VR player is facing. There is a slight offset to make the ellipse potrude more in front of the VR player than behind. This makes The AI stay farther awy from the front of the VR player, but allows it to get closer to the sides and back, where the VR player is not facing. This code can be found in [AvoidCloseVRAction.cs](/Src/Behaviors/Find%20Target%20Actions/AvoidCloseVRAction.cs).

- Laser beams shot by the VR player carve into the NavMesh after a short period of time determined by the AI. This allows the AI to walk into the laser beam and be sent to jail shortly after it has been shot, but avoid the laser beam after the NavMesh is carved. This is AI behavior emulating human reaction time. A carve into the NavMesh requires recalculation of the NavMesh nodes intersecting with the laser. After the laser beam disappears the NavMesh is restored to how it was orginally.

### Behavior Tree
All AI behavior is determined by a data structure called a [behavior tree](https://en.wikipedia.org/wiki/Behavior_tree_(artificial_intelligence,_robotics_and_control)). There are several different types of nodes in the behavior tree: logical nodes, conditional nodes, and action nodes. All of these nodes return a state ([BTNodeState.cs](/Src/BTNodeState.cs)): success, running, or failure. The two main types of logical nodes are selectors ([BTSelector.cs](/Src/BTSelector.cs)), which continue execution of all child nodes until one returns a state of success or running, and sequences ([BTSequence](/Src/BTSequence.cs)), which continue execution of all child nodes until one returns a state of failure. Conditional ([Conditionals/](/Src/Behaviors/Conditionals/)) nodes evaluate a condition and return a state based on the evaluation. Action nodes perform an AI behavior and make up the bulk of the nodes in the tree. There are also decorator nodes, which are child classes of action nodes that extend their behavior. All of these nodes allow the behavior tree to creating complex behavior for game AI and make it especially good for choosing tasks based on different levels of priority.

Our AI behavior tree is initialized in [AIController.cs](/Src/AIController.cs). This script is responsible for all hooking functions in the behavior tree to external game events and implements the interface for PC player movement and interaction.

The design of our game requires that most of the actions of consist of finding targets ([Find Target Actions/](/Src/Behaviors/Find%20Target%20Actions/)) based on priority and moving to targets ([Move Actions/](/Src/Behaviors/Move%20Actions/)). For example, if a **console** button can be pressed by the AI, it has a higher priority than choosing random target positions. If the **artifact** has no **barriers** active around it and can be reached by the AI, then it has a higher priority than pressing the button on a **console**. If the VR player is close to tagging the AI player and sending them to jail, the highest priority is moving to avoid the VR player rather than moving to the previous target position. 

We also created a group of TargetObjective ([Target Objective Types/](/Src/Behaviors/Target%20Objective%20Types/)) classes, which are used to convey information about the target, such as its position, what type of target it is, whether the AI knows its location, and more. TargetObjectives include the **relic**, **consoles**, **jails**, random positions, etc. Lists of available TargetObjectives are usually received by action nodes from external game events.
